{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiXMZ8DCGTAd"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dNCNaTyZByPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "N0mn8ahSRxj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ceLmdhiLCc2"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer,BertTokenizerFast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "8mPSONrgltIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be37a5e-965e-4d7d-d883-59e3c256d9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1AlroecLHQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af1e0d7-09cd-4772-ccdd-2f53499d5f15"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Loading the fine-tuned modeol\n",
        "model = torch.load(\"/content/drive/MyDrive/bertqa_finetuned/bertqa612\", map_location = torch.device('cpu'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbLzCx8FLN4K"
      },
      "source": [
        "def predict(query,context):\n",
        "\n",
        "  inputs = tokenizer.encode_plus(query, context, return_tensors='pt',truncation = True)\n",
        "  del inputs[\"token_type_ids\"]\n",
        "\n",
        "  outputs = model(**inputs)\n",
        "  answer_start = torch.argmax(outputs[0])\n",
        "  answer_end = torch.argmax(outputs[1]) + 1\n",
        "\n",
        "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
        "\n",
        "  return answer\n",
        "\n",
        "def normalize_text(s):\n",
        "  import string, re\n",
        "\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    return re.sub(regex, \" \", text)\n",
        "\n",
        "  def white_space_fix(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "  pred_tokens = normalize_text(prediction).split()\n",
        "  truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "    return int(pred_tokens == truth_tokens)\n",
        "\n",
        "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "  if len(common_tokens) == 0:\n",
        "    return 0\n",
        "\n",
        "  prec = len(common_tokens) / len(pred_tokens)\n",
        "  rec = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "  return 2 * (prec * rec) / (prec + rec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtMtke3JbCSR"
      },
      "source": [
        "def give_an_answer(context,query,answer):\n",
        "\n",
        "  prediction = predict(query,context)\n",
        "  if prediction == '[CLS]':\n",
        "    prediction = ''\n",
        "  em_score = compute_exact_match(prediction, answer)\n",
        "  f1_score = compute_f1(prediction, answer)\n",
        "  return em_score,f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path('/content/drive/MyDrive/data/test-v2.0.json')\n",
        "\n",
        "with open(path, 'rb') as f:\n",
        "    squad_dict = json.load(f)\n",
        "\n",
        "texts = []\n",
        "queries = []\n",
        "answers = []\n",
        "\n",
        "for group in squad_dict['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "        context = passage['context']\n",
        "        for qa in passage['qas']:\n",
        "            question = qa['question']\n",
        "            if(qa['answers'] == []):\n",
        "                texts.append(context)\n",
        "                queries.append(question)\n",
        "                answers.append({'text':'', 'answer_start':-1})\n",
        "            else:\n",
        "                for answer in qa['answers']:\n",
        "                    texts.append(context)\n",
        "                    queries.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "test_texts, test_queries, test_answers = texts, queries, answers"
      ],
      "metadata": {
        "id": "9xA4-LC6Br2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path('/content/drive/MyDrive/data/dev-v2.0.json')\n",
        "\n",
        "with open(path, 'rb') as f:\n",
        "    squad_dict = json.load(f)\n",
        "\n",
        "texts = []\n",
        "queries = []\n",
        "answers = []\n",
        "num = 0\n",
        "\n",
        "for group in squad_dict['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "        num += 1\n",
        "        context = passage['context']\n",
        "        for qa in passage['qas']:\n",
        "            question = qa['question']\n",
        "            for answer in qa['answers']:\n",
        "                texts.append(context)\n",
        "                queries.append(question)\n",
        "                answers.append(answer)\n",
        "\n",
        "val_texts, val_queries, val_answers = texts, queries, answers"
      ],
      "metadata": {
        "id": "bwXCDrUt9__T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_texts = val_texts[-2000:]\n",
        "val_queries = val_queries[-2000:]\n",
        "val_answers = val_answers[-2000:]"
      ],
      "metadata": {
        "id": "cFmYTdkE-KIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = test_texts[:2000]\n",
        "test_queries = test_queries[:2000]\n",
        "test_answers = test_answers[:2000]"
      ],
      "metadata": {
        "id": "ZKSK_441-K3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_em = 0\n",
        "val_f1 = 0\n",
        "for i in range(len(val_texts)):\n",
        "  em, f1 = give_an_answer(val_texts[i],val_queries[i],val_answers[i]['text'])\n",
        "  val_em = val_em + em\n",
        "  val_f1 = val_f1 + f1\n",
        "\n",
        "val_em = val_em/len(val_texts)\n",
        "val_f1 = val_f1/len(val_texts)\n",
        "print(\"Exact match score for queries with answers : \", val_em)\n",
        "print(\"F1-score for queries with answers : \", val_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D9TxnpW-PtP",
        "outputId": "fb80b838-2e52-4b0d-c429-9a5e188f2b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match score for queries with answers :  0.3095\n",
            "F1-score for queries with answers :  0.43103627170437514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_em = 0\n",
        "test_f1 = 0\n",
        "for i in range(len(test_texts)):\n",
        "  em, f1 = give_an_answer(test_texts[i],test_queries[i],test_answers[i]['text'])\n",
        "  test_em = test_em + em\n",
        "  test_f1 = test_f1 + f1\n",
        "\n",
        "test_em = test_em/len(test_texts)\n",
        "test_f1 = test_f1/len(test_texts)\n",
        "print(\"Exact match score for queries with no answers : \", test_em)\n",
        "print(\"F1-score for queries with no answers : \", test_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0SHxqSA-QiT",
        "outputId": "6b671b66-7f5e-45b4-ea77-d0df542f66d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match score for queries with no answers :  0.3685\n",
            "F1-score for queries with no answers :  0.3685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jv8CGK14v6Gr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}